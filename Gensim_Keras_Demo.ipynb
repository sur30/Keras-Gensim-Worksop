{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarora\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\sarora\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "from keras.layers import Input, Embedding, merge\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "vector_dim = 100\n",
    "root_path = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the input data into a list of integer indexes aligning with the wv indexes\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = f.read(f.namelist()[0]).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_data_to_index(string_data, wv):\n",
    "    index_data = []\n",
    "    for word in string_data:\n",
    "        if word in wv:\n",
    "            index_data.append(wv.vocab[word].index)\n",
    "    return index_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "## download the data \n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "filename = download_file('text8.zip', url, 31344016)\n",
    "if not os.path.exists((root_path +\"\\\\\"+ filename).strip('.zip')):\n",
    "    zipfile.ZipFile(root_path+\"\\\\\"+filename).extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-02 22:59:39,771 : INFO : collecting all words and their counts\n",
      "2018-02-02 22:59:39,774 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-02 22:59:44,930 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2018-02-02 22:59:44,931 : INFO : Loading a fresh vocabulary\n",
      "2018-02-02 22:59:45,225 : INFO : min_count=10 retains 47134 unique words (18% of original 253854, drops 206720)\n",
      "2018-02-02 22:59:45,225 : INFO : min_count=10 leaves 16561031 word corpus (97% of original 17005207, drops 444176)\n",
      "2018-02-02 22:59:45,402 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2018-02-02 22:59:45,417 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2018-02-02 22:59:45,418 : INFO : downsampling leaves estimated 12333563 word corpus (74.5% of prior 16561031)\n",
      "2018-02-02 22:59:45,419 : INFO : estimated required memory for 47134 words and 100 dimensions: 61274200 bytes\n",
      "2018-02-02 22:59:45,636 : INFO : resetting layer weights\n",
      "2018-02-02 22:59:46,864 : INFO : training model with 4 workers on 47134 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-02 22:59:47,888 : INFO : PROGRESS: at 0.49% examples, 596065 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 22:59:48,913 : INFO : PROGRESS: at 1.01% examples, 604403 words/s, in_qsize 7, out_qsize 2\n",
      "2018-02-02 22:59:49,915 : INFO : PROGRESS: at 1.58% examples, 635551 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 22:59:50,923 : INFO : PROGRESS: at 2.06% examples, 624391 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 22:59:51,929 : INFO : PROGRESS: at 2.60% examples, 633565 words/s, in_qsize 7, out_qsize 2\n",
      "2018-02-02 22:59:52,956 : INFO : PROGRESS: at 3.19% examples, 646083 words/s, in_qsize 4, out_qsize 2\n",
      "2018-02-02 22:59:53,986 : INFO : PROGRESS: at 3.76% examples, 653444 words/s, in_qsize 8, out_qsize 1\n",
      "2018-02-02 22:59:55,005 : INFO : PROGRESS: at 4.44% examples, 675342 words/s, in_qsize 7, out_qsize 2\n",
      "2018-02-02 22:59:56,014 : INFO : PROGRESS: at 5.16% examples, 697079 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 22:59:57,022 : INFO : PROGRESS: at 5.83% examples, 709644 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 22:59:58,023 : INFO : PROGRESS: at 6.54% examples, 724786 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-02 22:59:59,025 : INFO : PROGRESS: at 7.21% examples, 733976 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:00:00,043 : INFO : PROGRESS: at 7.93% examples, 743206 words/s, in_qsize 4, out_qsize 1\n",
      "2018-02-02 23:00:01,045 : INFO : PROGRESS: at 8.65% examples, 753430 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:02,046 : INFO : PROGRESS: at 9.35% examples, 759924 words/s, in_qsize 5, out_qsize 2\n",
      "2018-02-02 23:00:03,048 : INFO : PROGRESS: at 10.04% examples, 765004 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:04,052 : INFO : PROGRESS: at 10.76% examples, 772147 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:05,056 : INFO : PROGRESS: at 11.46% examples, 776673 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:06,060 : INFO : PROGRESS: at 12.18% examples, 782246 words/s, in_qsize 2, out_qsize 0\n",
      "2018-02-02 23:00:07,061 : INFO : PROGRESS: at 12.86% examples, 785616 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:08,062 : INFO : PROGRESS: at 13.56% examples, 789419 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:09,063 : INFO : PROGRESS: at 14.14% examples, 786572 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:00:10,068 : INFO : PROGRESS: at 14.85% examples, 790219 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:11,074 : INFO : PROGRESS: at 15.51% examples, 791066 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:00:12,079 : INFO : PROGRESS: at 16.21% examples, 793871 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:13,080 : INFO : PROGRESS: at 16.85% examples, 794092 words/s, in_qsize 7, out_qsize 2\n",
      "2018-02-02 23:00:14,086 : INFO : PROGRESS: at 17.55% examples, 796029 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:00:15,090 : INFO : PROGRESS: at 18.26% examples, 798303 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:00:16,119 : INFO : PROGRESS: at 18.95% examples, 799293 words/s, in_qsize 8, out_qsize 2\n",
      "2018-02-02 23:00:17,125 : INFO : PROGRESS: at 19.65% examples, 801313 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-02 23:00:18,130 : INFO : PROGRESS: at 20.30% examples, 801034 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-02 23:00:19,132 : INFO : PROGRESS: at 20.99% examples, 802283 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:00:20,133 : INFO : PROGRESS: at 21.68% examples, 803365 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:21,140 : INFO : PROGRESS: at 22.35% examples, 803977 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:00:22,141 : INFO : PROGRESS: at 23.03% examples, 805487 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:23,151 : INFO : PROGRESS: at 23.72% examples, 806508 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:00:24,164 : INFO : PROGRESS: at 24.39% examples, 806818 words/s, in_qsize 5, out_qsize 3\n",
      "2018-02-02 23:00:25,178 : INFO : PROGRESS: at 25.09% examples, 808132 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:26,188 : INFO : PROGRESS: at 25.77% examples, 809023 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:27,196 : INFO : PROGRESS: at 26.43% examples, 808815 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:00:28,197 : INFO : PROGRESS: at 27.08% examples, 808784 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:29,197 : INFO : PROGRESS: at 27.78% examples, 809902 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-02 23:00:30,208 : INFO : PROGRESS: at 28.48% examples, 810702 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:00:31,211 : INFO : PROGRESS: at 29.14% examples, 810659 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:32,220 : INFO : PROGRESS: at 29.83% examples, 811350 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-02 23:00:33,228 : INFO : PROGRESS: at 30.51% examples, 811677 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:00:34,236 : INFO : PROGRESS: at 31.20% examples, 812226 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:35,242 : INFO : PROGRESS: at 31.88% examples, 812717 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:36,250 : INFO : PROGRESS: at 32.56% examples, 813127 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:37,254 : INFO : PROGRESS: at 33.23% examples, 813534 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:38,266 : INFO : PROGRESS: at 33.89% examples, 813593 words/s, in_qsize 2, out_qsize 0\n",
      "2018-02-02 23:00:39,273 : INFO : PROGRESS: at 34.56% examples, 813813 words/s, in_qsize 7, out_qsize 2\n",
      "2018-02-02 23:00:40,283 : INFO : PROGRESS: at 35.24% examples, 814205 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:00:41,293 : INFO : PROGRESS: at 35.91% examples, 814369 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:00:42,294 : INFO : PROGRESS: at 36.55% examples, 813950 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:43,298 : INFO : PROGRESS: at 37.25% examples, 814710 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:00:44,308 : INFO : PROGRESS: at 37.90% examples, 814116 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:00:45,338 : INFO : PROGRESS: at 38.62% examples, 814994 words/s, in_qsize 1, out_qsize 1\n",
      "2018-02-02 23:00:46,349 : INFO : PROGRESS: at 39.31% examples, 815356 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:00:47,353 : INFO : PROGRESS: at 39.98% examples, 815325 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:00:48,364 : INFO : PROGRESS: at 40.68% examples, 815897 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:00:49,365 : INFO : PROGRESS: at 41.35% examples, 816003 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:00:50,377 : INFO : PROGRESS: at 42.03% examples, 816098 words/s, in_qsize 2, out_qsize 1\n",
      "2018-02-02 23:00:51,384 : INFO : PROGRESS: at 42.70% examples, 816302 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:52,391 : INFO : PROGRESS: at 43.37% examples, 816661 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-02 23:00:53,392 : INFO : PROGRESS: at 44.04% examples, 816750 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:00:54,394 : INFO : PROGRESS: at 44.70% examples, 816830 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:00:55,412 : INFO : PROGRESS: at 45.37% examples, 816787 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:00:56,418 : INFO : PROGRESS: at 46.06% examples, 817137 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-02 23:00:57,423 : INFO : PROGRESS: at 46.72% examples, 817131 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:00:58,424 : INFO : PROGRESS: at 47.38% examples, 817230 words/s, in_qsize 5, out_qsize 1\n",
      "2018-02-02 23:00:59,443 : INFO : PROGRESS: at 48.08% examples, 817383 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:00,451 : INFO : PROGRESS: at 48.75% examples, 817382 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:01,465 : INFO : PROGRESS: at 49.43% examples, 817422 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:02,465 : INFO : PROGRESS: at 49.99% examples, 815739 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:01:03,468 : INFO : PROGRESS: at 50.67% examples, 815848 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:01:04,473 : INFO : PROGRESS: at 51.29% examples, 815133 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:05,477 : INFO : PROGRESS: at 51.98% examples, 815403 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:06,482 : INFO : PROGRESS: at 52.65% examples, 815580 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:07,482 : INFO : PROGRESS: at 53.22% examples, 814389 words/s, in_qsize 2, out_qsize 0\n",
      "2018-02-02 23:01:08,484 : INFO : PROGRESS: at 53.91% examples, 814868 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:09,493 : INFO : PROGRESS: at 54.50% examples, 813822 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:10,503 : INFO : PROGRESS: at 55.16% examples, 813733 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:11,504 : INFO : PROGRESS: at 55.83% examples, 813924 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:12,512 : INFO : PROGRESS: at 56.48% examples, 813760 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:01:13,513 : INFO : PROGRESS: at 57.15% examples, 813886 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:14,548 : INFO : PROGRESS: at 57.84% examples, 813845 words/s, in_qsize 2, out_qsize 4\n",
      "2018-02-02 23:01:15,551 : INFO : PROGRESS: at 58.53% examples, 814189 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:01:16,556 : INFO : PROGRESS: at 59.19% examples, 814168 words/s, in_qsize 6, out_qsize 2\n",
      "2018-02-02 23:01:17,558 : INFO : PROGRESS: at 59.88% examples, 814390 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:18,578 : INFO : PROGRESS: at 60.53% examples, 814173 words/s, in_qsize 7, out_qsize 2\n",
      "2018-02-02 23:01:19,590 : INFO : PROGRESS: at 61.20% examples, 814004 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:20,593 : INFO : PROGRESS: at 61.88% examples, 814271 words/s, in_qsize 2, out_qsize 2\n",
      "2018-02-02 23:01:21,597 : INFO : PROGRESS: at 62.56% examples, 814494 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:22,603 : INFO : PROGRESS: at 63.22% examples, 814530 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:23,609 : INFO : PROGRESS: at 63.83% examples, 813987 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:24,615 : INFO : PROGRESS: at 64.50% examples, 814106 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:01:25,636 : INFO : PROGRESS: at 65.18% examples, 814167 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:26,650 : INFO : PROGRESS: at 65.86% examples, 814289 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:01:27,654 : INFO : PROGRESS: at 66.53% examples, 814412 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:28,661 : INFO : PROGRESS: at 67.17% examples, 814152 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:01:29,662 : INFO : PROGRESS: at 67.86% examples, 814404 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:30,665 : INFO : PROGRESS: at 68.54% examples, 814605 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:31,670 : INFO : PROGRESS: at 69.21% examples, 814665 words/s, in_qsize 5, out_qsize 0\n",
      "2018-02-02 23:01:32,694 : INFO : PROGRESS: at 69.85% examples, 814137 words/s, in_qsize 7, out_qsize 2\n",
      "2018-02-02 23:01:33,697 : INFO : PROGRESS: at 70.56% examples, 814675 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:01:34,698 : INFO : PROGRESS: at 71.23% examples, 814760 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:35,708 : INFO : PROGRESS: at 71.89% examples, 814596 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:36,708 : INFO : PROGRESS: at 72.57% examples, 814962 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:37,715 : INFO : PROGRESS: at 73.25% examples, 815182 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:01:38,716 : INFO : PROGRESS: at 73.93% examples, 815391 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:01:39,721 : INFO : PROGRESS: at 74.60% examples, 815559 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:40,722 : INFO : PROGRESS: at 75.26% examples, 815534 words/s, in_qsize 2, out_qsize 1\n",
      "2018-02-02 23:01:41,725 : INFO : PROGRESS: at 75.93% examples, 815577 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:42,729 : INFO : PROGRESS: at 76.57% examples, 815414 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:43,732 : INFO : PROGRESS: at 77.22% examples, 815278 words/s, in_qsize 6, out_qsize 2\n",
      "2018-02-02 23:01:44,750 : INFO : PROGRESS: at 77.93% examples, 815541 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:45,759 : INFO : PROGRESS: at 78.58% examples, 815321 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:46,759 : INFO : PROGRESS: at 79.25% examples, 815387 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:47,773 : INFO : PROGRESS: at 79.91% examples, 815287 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:01:48,775 : INFO : PROGRESS: at 80.58% examples, 815330 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:49,779 : INFO : PROGRESS: at 81.24% examples, 815210 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:50,782 : INFO : PROGRESS: at 81.93% examples, 815517 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:51,807 : INFO : PROGRESS: at 82.60% examples, 815500 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:52,828 : INFO : PROGRESS: at 83.28% examples, 815600 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:01:53,829 : INFO : PROGRESS: at 83.94% examples, 815670 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:54,833 : INFO : PROGRESS: at 84.61% examples, 815747 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:01:55,842 : INFO : PROGRESS: at 85.27% examples, 815681 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:01:56,847 : INFO : PROGRESS: at 85.94% examples, 815707 words/s, in_qsize 2, out_qsize 0\n",
      "2018-02-02 23:01:57,853 : INFO : PROGRESS: at 86.62% examples, 815883 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:01:58,858 : INFO : PROGRESS: at 87.29% examples, 815967 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:01:59,866 : INFO : PROGRESS: at 88.01% examples, 816302 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:02:00,888 : INFO : PROGRESS: at 88.67% examples, 816132 words/s, in_qsize 5, out_qsize 2\n",
      "2018-02-02 23:02:01,891 : INFO : PROGRESS: at 89.36% examples, 816378 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:02:02,911 : INFO : PROGRESS: at 90.05% examples, 816526 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:02:03,915 : INFO : PROGRESS: at 90.75% examples, 816720 words/s, in_qsize 6, out_qsize 1\n",
      "2018-02-02 23:02:04,928 : INFO : PROGRESS: at 91.44% examples, 816895 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:02:05,941 : INFO : PROGRESS: at 92.12% examples, 817003 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:02:06,942 : INFO : PROGRESS: at 92.80% examples, 817184 words/s, in_qsize 7, out_qsize 1\n",
      "2018-02-02 23:02:07,945 : INFO : PROGRESS: at 93.46% examples, 817259 words/s, in_qsize 6, out_qsize 0\n",
      "2018-02-02 23:02:08,961 : INFO : PROGRESS: at 94.17% examples, 817559 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:02:09,979 : INFO : PROGRESS: at 94.60% examples, 815514 words/s, in_qsize 7, out_qsize 4\n",
      "2018-02-02 23:02:10,987 : INFO : PROGRESS: at 95.22% examples, 815107 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:02:11,998 : INFO : PROGRESS: at 95.79% examples, 814307 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-02 23:02:13,016 : INFO : PROGRESS: at 96.33% examples, 813164 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:02:14,021 : INFO : PROGRESS: at 96.88% examples, 812309 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-02 23:02:15,029 : INFO : PROGRESS: at 97.45% examples, 811501 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:02:16,037 : INFO : PROGRESS: at 98.01% examples, 810562 words/s, in_qsize 7, out_qsize 0\n",
      "2018-02-02 23:02:17,062 : INFO : PROGRESS: at 98.57% examples, 809610 words/s, in_qsize 6, out_qsize 2\n",
      "2018-02-02 23:02:18,081 : INFO : PROGRESS: at 99.13% examples, 808701 words/s, in_qsize 6, out_qsize 2\n",
      "2018-02-02 23:02:19,086 : INFO : PROGRESS: at 99.84% examples, 809027 words/s, in_qsize 8, out_qsize 0\n",
      "2018-02-02 23:02:19,335 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-02 23:02:19,344 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-02 23:02:19,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-02 23:02:19,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-02 23:02:19,352 : INFO : training on 170052070 raw words (123341708 effective words) took 152.5s, 808939 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:40.415000\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "tstart = datetime.now()\n",
    "sentences = word2vec.Text8Corpus((root_path +\"\\\\\"+ filename).strip('.zip'))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = word2vec.Word2Vec(sentences, iter=10, min_count=10, size=100, workers=4)\n",
    "tend = datetime.now()\n",
    "print (tend - tstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-02 23:03:12,158 : INFO : saving Word2Vec object under C:\\Users\\sarora\\Documents\\workshop\\Keras_Gensim_Workshopw2v_gensim.model, separately None\n",
      "2018-02-02 23:03:12,158 : INFO : not storing attribute syn0norm\n",
      "2018-02-02 23:03:12,159 : INFO : not storing attribute cum_table\n",
      "2018-02-02 23:03:13,167 : INFO : saved C:\\Users\\sarora\\Documents\\workshop\\Keras_Gensim_Workshopw2v_gensim.model\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save(root_path + \"w2v_gensim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get the word vector\n",
    "word_vectors = model.wv\n",
    "## delete the model to free RAM space\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3061564  -0.96115386 -0.8630267   1.8129046  -0.06767506  0.92952305\n",
      "  0.3761787  -1.4928774   1.3614581  -1.4495217  -0.28127846  0.2622818\n",
      " -0.54526836  1.2730536  -0.6922045   0.2692608   0.94456685 -0.06606608\n",
      "  0.22785585 -0.26709765 -1.1088084   1.0772316   2.0432854   1.8930763\n",
      "  0.30403492  0.7586897  -0.493812   -0.01467971 -0.22679994  1.3555989\n",
      " -1.6310763  -0.42341593  0.3056518  -0.46745062 -0.83427227  0.03583196\n",
      " -1.1058726  -0.26517883  0.60105604 -0.91067505 -0.64660984 -0.7610009\n",
      " -0.8554591   0.7420312  -0.8882003  -1.1422915   2.2729921  -0.1819321\n",
      "  0.48678255  0.5457513  -0.62141335  0.4376641  -0.02157935 -0.45318267\n",
      "  0.0508013   2.344787    0.04180916 -0.74063176 -0.39010966  1.0210367\n",
      " -0.81849724 -2.2233632  -0.2897524   0.07855511  0.8952815  -0.89665014\n",
      "  0.48190504 -0.47030613  0.5526116  -0.7043509  -1.3606175  -0.66960335\n",
      " -1.1854233   1.2719721  -1.137403    2.2702281   2.0156488  -0.21478519\n",
      "  0.49565825  0.05780362  1.4134271   0.05242933 -0.5266564   0.5709386\n",
      "  0.49262598 -1.0364374   0.96877545 -1.1038584  -2.1520855  -0.93697923\n",
      "  0.792064    2.1940026  -0.9627335  -0.7728356  -0.3482356  -2.0173957\n",
      " -1.1141336  -0.0473652  -1.361824   -0.39558223]\n"
     ]
    }
   ],
   "source": [
    "# get the word vector of \"of\"\n",
    "print(word_vectors['of'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the of and\n"
     ]
    }
   ],
   "source": [
    "# get the most common words\n",
    "print(word_vectors.index2word[0], word_vectors.index2word[1], word_vectors.index2word[2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zechs lilies campylobacter\n"
     ]
    }
   ],
   "source": [
    "# get the least common words\n",
    "vocab_size = len(word_vectors.vocab)\n",
    "print(word_vectors.index2word[vocab_size - 1], word_vectors.index2word[vocab_size - 2], word_vectors.index2word[vocab_size - 3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 1.1354467868804932),\n",
       " ('singer', 1.0441545248031616),\n",
       " ('playwright', 1.0072388648986816),\n",
       " ('judi', 0.9815788269042969),\n",
       " ('entertainer', 0.9794433116912842),\n",
       " ('comedienne', 0.9782858490943909),\n",
       " ('musician', 0.9759577512741089),\n",
       " ('novelist', 0.9746918082237244),\n",
       " ('ballerina', 0.9724231958389282),\n",
       " ('comedian', 0.9664108157157898)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get female for actor\n",
    "word_vectors.most_similar_cosmul(positive=['woman', 'actor'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between woman and man =  0.7183470966698886\n",
      "Similarity between man and girl =  0.6545588697169208\n",
      "Similarity between man and tiger =  0.15145760451573023\n",
      "Similarity between man and fox =  0.1565765262268731\n"
     ]
    }
   ],
   "source": [
    "# similarity check\n",
    "print (\"Similarity between woman and man = \",word_vectors.similarity('woman', 'man'))\n",
    "print (\"Similarity between man and girl = \" ,word_vectors.similarity('man', 'girl'))\n",
    "print (\"Similarity between man and tiger = \",word_vectors.similarity('man', 'tiger'))\n",
    "print (\"Similarity between man and fox = \",word_vectors.similarity('man', 'fox'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zebra\n"
     ]
    }
   ],
   "source": [
    "#odd one out\n",
    "print(word_vectors.doesnt_match(\"green blue red zebra\".split()))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS using word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(model):\n",
    "    # convert the wv word vectors into a numpy matrix that is suitable for insertion\n",
    "    # into our Keras models\n",
    "    embedding_matrix = np.zeros((len(word_vectors.vocab), vector_dim))\n",
    "    for i in range(len(word_vectors.vocab)):\n",
    "        embedding_vector = word_vectors[word_vectors.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = gensim.models.Word2Vec.load(root_path + \"\\\\w2v_gensim.model\")\n",
    "embedding_matrix = create_embedding_matrix(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_size = 5  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "# input words - in this case we do sample by sample evaluations of the similarity\n",
    "valid_word = Input((1,), dtype='int32')\n",
    "other_word = Input((1,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarora\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Users\\sarora\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\Users\\sarora\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"me..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# setup the embedding layer\n",
    "embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "                  weights=[embedding_matrix])\n",
    "embedded_a = embeddings(valid_word)\n",
    "embedded_b = embeddings(other_word)\n",
    "similarity = merge([embedded_a, embedded_b], mode='cos', dot_axes=2)\n",
    "# create the Keras model\n",
    "k_model = Model(input=[valid_word, other_word], output=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim(valid_word_idx, vocab_size):\n",
    "    sim = np.zeros((vocab_size,))\n",
    "    in_arr1 = np.zeros((1,))\n",
    "    in_arr2 = np.zeros((1,))\n",
    "    in_arr1[0,] = valid_word_idx\n",
    "    for i in range(vocab_size):\n",
    "        in_arr2[0,] = i\n",
    "        out = k_model.predict_on_batch([in_arr1, in_arr2])\n",
    "        sim[i] = out\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to his: her, him, himself, faramir, my, their,\n",
      "Nearest to no: nothing, any, none, little, neither, only,\n",
      "Nearest to between: irreconcilable, among, across, within, respectively, with,\n",
      "Nearest to they: we, you, themselves, them, theirs, these,\n",
      "Nearest to from: back, through, onto, across, periodically, via,\n"
     ]
    }
   ],
   "source": [
    "word_vectors\n",
    "# now run the model and get the closest words to the valid examples\n",
    "for i in range(valid_size):\n",
    "    valid_word = word_vectors.index2word[valid_examples[i]]\n",
    "    top_k = 6  # number of nearest neighbors\n",
    "    sim = get_sim(valid_examples[i], len(word_vectors.vocab))\n",
    "    nearest = (-sim).argsort()[1:top_k + 1]\n",
    "    log_str = 'Nearest to %s:' % valid_word\n",
    "    for k in range(top_k):\n",
    "        close_word = word_vectors.index2word[nearest[k]]\n",
    "        log_str = '%s %s,' % (log_str, close_word)\n",
    "    print(log_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
